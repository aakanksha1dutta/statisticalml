{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  nltk.stem  import  WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re, string\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1159 entries, 0 to 1158\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   title   1077 non-null   object\n",
      " 1   text    1151 non-null   object\n",
      " 2   source  1142 non-null   object\n",
      " 3   label   1159 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 36.3+ KB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('corona_fake.csv')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordnetlemmatizer uses their own pos tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN \n",
    "\n",
    "def strip_emoji(text):\n",
    "    RE_EMOJI = re.compile(u'([\\U00002600-\\U000027BF])|([\\U0001f300-\\U0001f64F])|([\\U0001f680-\\U0001f6FF])')\n",
    "    return RE_EMOJI.sub(r'', text)\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    lemmatized_tokens = []\n",
    "    lemmmatizer = WordNetLemmatizer()\n",
    "    for token, tag in tagged:\n",
    "        if tag:\n",
    "            lemmatized_tokens.append(lemmmatizer.lemmatize(word=token, pos=get_wordnet_pos(tag)))\n",
    "        else:\n",
    "            lemmatized_tokens.append(lemmmatizer.lemmatize(token))\n",
    "    \n",
    "    tokens = [word for word in lemmatized_tokens if word not in stop_words]\n",
    "\n",
    "    #remove  numbers,  words  that  are  shorter  than  2 characters, punctuation, links and emojis.\n",
    "    tokens = [re.sub(r\"\\b[0-9]+\\b\\s*\", '', text) for text in tokens]\n",
    "    translator = str.maketrans(\"\",\"\", string.punctuation)\n",
    "    tokens = [text.translate(translator) for text in tokens]\n",
    "    tokens = [re.sub(r\"https?://\\S+\", \"\", text) for text in tokens]\n",
    "    tokens = [re.sub(r\"[^A-Za-z0-9\\s]+\", \"\", text) for text in tokens] #also remove special chars if any\n",
    "    tokens = [strip_emoji(text) for text in tokens]\n",
    "    tokens = [text for text in tokens if len(text)>2]\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_clean'] = data['text'].astype(str).apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a:\n",
    "An N Gram is a sequence of N adjacent symbols, with a specified order.\n",
    "\n",
    "In the context of NLP, N-gram captures multi word semantics and word order which is important for models like Bag of Words to represent the corpus correctly.\n",
    "\n",
    "## b:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvectorizer1 = CountVectorizer(ngram_range=(1,1), lowercase=True)\n",
    "cvectorizer2 = CountVectorizer(ngram_range=(1,2), lowercase=True)\n",
    "cvectorizer3 = CountVectorizer(ngram_range=(1,3), lowercase=True)\n",
    "\n",
    "vector1 = cvectorizer1.fit_transform(data['text_clean'])\n",
    "vector2 = cvectorizer2.fit_transform(data['text_clean'])\n",
    "vector3 = cvectorizer3.fit_transform(data['text_clean'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfid1 = TfidfVectorizer(ngram_range=(1,1), lowercase=True)\n",
    "tfid2 = TfidfVectorizer(ngram_range=(1,2), lowercase=True)\n",
    "tfid3 = TfidfVectorizer(ngram_range=(1,3), lowercase=True)\n",
    "\n",
    "tfvector1 = tfid1.fit_transform(data['text_clean'])\n",
    "tfvector2 = tfid2.fit_transform(data['text_clean'])\n",
    "tfvector3 = tfid3.fit_transform(data['text_clean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Count Vectorizer with N-Gram (1, 1):0.882183908045977\n",
      "Accuracy of Count Vectorizer with N-Gram (1, 2):0.8936781609195402\n",
      "Accuracy of Count Vectorizer with N-Gram (1, 3):0.8879310344827587\n",
      "Accuracy of TfIDf Vectorizer with N-Gram (1, 1):0.9224137931034483\n",
      "Accuracy of TfIDf Vectorizer with N-Gram (1, 2):0.9137931034482759\n",
      "Accuracy of TfIDf Vectorizer with N-Gram (1, 3):0.9080459770114943\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "target = data['label']\n",
    "vectors = [vector1, vector2, vector3, tfvector1, tfvector2, tfvector3]\n",
    "methods = [f'Count Vectorizer with N-Gram (1, {i})' for i in range(1,4)]+[f'TfIDf Vectorizer with N-Gram (1, {i})' for i in range(1,4)]\n",
    "accuracies = []\n",
    "for vec, method in zip(vectors, methods):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(vec, target, train_size=0.7, random_state=276)\n",
    "    reg = LogisticRegressionCV(cv = 5, random_state = 265,  max_iter = 1000,  n_jobs = -1)\n",
    "    reg.fit(X_train, y_train)\n",
    "    y_pred = reg.predict(X_test)\n",
    "    acc = accuracy_score(y_test,y_pred)\n",
    "    print(f\"Accuracy of {method}:{acc}\")\n",
    "    accuracies.append(acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Accuracy Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Count Vectorizer with N-Gram (1, 1)</td>\n",
       "      <td>0.882184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Count Vectorizer with N-Gram (1, 2)</td>\n",
       "      <td>0.893678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Count Vectorizer with N-Gram (1, 3)</td>\n",
       "      <td>0.887931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TfIDf Vectorizer with N-Gram (1, 1)</td>\n",
       "      <td>0.922414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TfIDf Vectorizer with N-Gram (1, 2)</td>\n",
       "      <td>0.913793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TfIDf Vectorizer with N-Gram (1, 3)</td>\n",
       "      <td>0.908046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Method  Accuracy Score\n",
       "0  Count Vectorizer with N-Gram (1, 1)        0.882184\n",
       "1  Count Vectorizer with N-Gram (1, 2)        0.893678\n",
       "2  Count Vectorizer with N-Gram (1, 3)        0.887931\n",
       "3  TfIDf Vectorizer with N-Gram (1, 1)        0.922414\n",
       "4  TfIDf Vectorizer with N-Gram (1, 2)        0.913793\n",
       "5  TfIDf Vectorizer with N-Gram (1, 3)        0.908046"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_df = pd.DataFrame({'Method': methods, 'Accuracy Score': accuracies})\n",
    "acc_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a :  Newton-CG (Newton‚Äôs Conjugate Gradient Method)\n",
    "**Newton‚Äôs method** approximates roots of  ùëì(ùë•)=0  by starting with an initial approximation  ùë•0 , then uses tangent lines to the graph of  ùëì  to update its approximations, powered by the first and second derivative of the function. This is then used to try to minimize the log likelihood which helps estimate the distribution parameters through MLE.\n",
    "\n",
    "$$\n",
    "\\beta^{(t+1)} = \\beta^{(t)} - H^{-1} \\nabla L(\\beta)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( \\beta \\) represents the parameter vector.\n",
    "- \\( H \\) is the Hessian matrix (second-order partial derivatives of the loss function).\n",
    "- $\\nabla L(\\beta)$ is the gradient of the loss function.\n",
    "\n",
    "Specifially, in logistic regression, the `newton-cg` solver is used, which is based on the Newton‚Äôs method combined with conjugate gradient descent. Conjugate Gradient inverts the Hessian matrix iteratively, which is faster and more memory-efficient than a direct inversion. It works especially well when the dataset is large, and computing the full Hessian explicitly is expensive.\n",
    "\n",
    "_____\n",
    "\n",
    "\n",
    "\n",
    "## b. L-BFGS (Limited-memory Broyden-Fletcher-Goldfarb-Shanno)\n",
    "`lbfgs` is a quasi-Newton optimization algorithm that approximates the second-order derivatives without calculating the full Hessian matrix. Like other optimization algorithms, it calculates the gradient of the log-likelihood function but instead of calculating the exact second derivatives, it uses the last m gradient updates to estimate it.\n",
    "\n",
    "L-BFGS approximates the inverse Hessian using past gradients and updates the parameters:\n",
    "\n",
    "$$\n",
    "\\beta^{(t+1)} = \\beta^{(t)} - \\alpha H^{-1} \\nabla L(\\beta)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ is the learning rate.\n",
    "- $H^{-1}$ is the approximated inverse Hessian matrix.\n",
    "- $\\nabla L(\\beta)$ is the gradient.\n",
    "\n",
    "---\n",
    "\n",
    "## c. Liblinear (Coordinate Descent for L1 and L2 Penalties)\n",
    "\n",
    "It‚Äôs based on the LIBLINEAR library‚Äîspecifically designed for large-scale linear classification. \n",
    "\n",
    "For L1 Regularization: It uses a coordinate descent method, which updates one parameter at a time while keeping others fixed. This is efficient for enforcing sparsity (many parameters being exactly zero).\n",
    "\n",
    "For L2 Regularization: It uses a trust region Newton method‚Äîa variation of Newton‚Äôs method that restricts updates to a safe region to avoid overshooting.\n",
    "\n",
    "One-vs-Rest (OvR) for Multi-Class: Since it cannot handle multinomial loss directly, it fits one binary model per class and combines them.\n",
    "Liblinear solves the optimization problem using coordinate-wise updates:\n",
    "\n",
    "$$\n",
    "\\min_{\\beta} \\left( L(\\beta) + \\lambda \\|\\beta\\|_1 + \\frac{1}{2} \\lambda_2 \\|\\beta\\|_2^2 \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $L(\\beta)$ is the loss function (log-loss for logistic regression).\n",
    "- $\\lambda \\|\\beta\\|_1$ is the L1 regularization term (for sparsity).\n",
    "- $\\lambda_2 \\|\\beta\\|_2^2$ is the L2 regularization term (for weight shrinkage).\n",
    "\n",
    "---\n",
    "\n",
    "## d. SAG (Stochastic Average Gradient)\n",
    "SAG is a variance-reduced version of Stochastic Gradient Descent (SGD). Instead of updating parameters based on a single random sample, it keeps track of and averages all previous gradients, improving convergence speed and stability. Due to averaging, there is fast convergence on large and high-dimensional data. It only works with L2 regularization. In SAG, each update relies on the gradient of a single randomly chosen sample, combined with the average of previous gradients. This leads to biased gradient estimates.\n",
    "\n",
    "$$\n",
    "\\beta^{(t+1)} = \\beta^{(t)} - \\eta \\left( \\bar{g} \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\eta$ is the learning rate.\n",
    "- $g_i$ is the gradient of the \\( i \\)-th sample.\n",
    "- $\\bar{g} = \\frac{1}{n} \\sum_{i=1}^{n} g_i $ is the average of all stored gradients.\n",
    "- $n$ is the number of data points.\n",
    "\n",
    "---\n",
    "\n",
    "## e. SAGA (SAG with Variance Reduction)\n",
    "\n",
    "SAGA is an improved version of SAG that adds support for non-smooth penalties (like L1). It corrects bias of SAG to keep estiimators unbiased. It uses updates to manage L1 regularization, which can force some coefficients to zero to keep data sparsity.\n",
    "\n",
    "$$\n",
    "\\beta^{(t+1)} = \\beta^{(t)} - \\eta \\left( \\nabla L_i(\\beta) - g_i + \\bar{g} \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $g_i$ is the stored gradient for the $i$-th sample.\n",
    "- $\\bar{g}$ is the average gradient across all samples.\n",
    "- $\\eta$ is the learning rate.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

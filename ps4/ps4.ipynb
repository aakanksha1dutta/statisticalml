{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 4 \n",
    "\n",
    "\n",
    "___\n",
    "Aakanksha Dutta [adutta5@u.rochester.edu](adutta5@u.rochester.edu) and Aabha Pandit [apandit@u.rochester.edu](apandit@u.rochester.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import eig\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1483 entries, 0 to 1482\n",
      "Columns: 148 entries, age to cat_no_of_pedestrians_on_other_lane_saved\n",
      "dtypes: int64(139), object(9)\n",
      "memory usage: 1.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>grown_up_in_US</th>\n",
       "      <th>grown_up_in_a_foreign_country</th>\n",
       "      <th>city_of_origin</th>\n",
       "      <th>state_of_origin</th>\n",
       "      <th>region_of_origin</th>\n",
       "      <th>country_of_origin</th>\n",
       "      <th>no_of_sisters</th>\n",
       "      <th>no_of_brothers</th>\n",
       "      <th>...</th>\n",
       "      <th>dog_no_of_pedestrians_on_other_lane_died</th>\n",
       "      <th>dog_no_of_passengers_saved</th>\n",
       "      <th>dog_no_of_pedestrians_on_lane_ahead_saved</th>\n",
       "      <th>dog_no_of_pedestrians_on_other_lane_saved</th>\n",
       "      <th>cat_no_of_passengers_died</th>\n",
       "      <th>cat_no_of_pedestrians_on_lane_ahead_died</th>\n",
       "      <th>cat_no_of_pedestrians_on_other_lane_died</th>\n",
       "      <th>cat_no_of_passengers_saved</th>\n",
       "      <th>cat_no_of_pedestrians_on_lane_ahead_saved</th>\n",
       "      <th>cat_no_of_pedestrians_on_other_lane_saved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21</td>\n",
       "      <td>m</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>kolkata</td>\n",
       "      <td>west bengal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>india</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>m</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>kolkata</td>\n",
       "      <td>west bengal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>india</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>m</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>kolkata</td>\n",
       "      <td>west bengal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>india</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>m</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>kolkata</td>\n",
       "      <td>west bengal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>india</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>m</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>kolkata</td>\n",
       "      <td>west bengal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>india</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1478</th>\n",
       "      <td>22</td>\n",
       "      <td>m</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>changde</td>\n",
       "      <td>hunan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>china</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1479</th>\n",
       "      <td>22</td>\n",
       "      <td>m</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>changde</td>\n",
       "      <td>hunan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>china</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1480</th>\n",
       "      <td>22</td>\n",
       "      <td>m</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>changde</td>\n",
       "      <td>hunan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>china</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>22</td>\n",
       "      <td>m</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>changde</td>\n",
       "      <td>hunan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>china</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>22</td>\n",
       "      <td>m</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>changde</td>\n",
       "      <td>hunan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>china</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1483 rows × 148 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age gender   grown_up_in_US   grown_up_in_a_foreign_country   \\\n",
       "0      21       m                0                               1   \n",
       "1      21       m                0                               1   \n",
       "2      21       m                0                               1   \n",
       "3      21       m                0                               1   \n",
       "4      21       m                0                               1   \n",
       "...   ...     ...              ...                             ...   \n",
       "1478   22       m                0                               1   \n",
       "1479   22       m                0                               1   \n",
       "1480   22       m                0                               1   \n",
       "1481   22       m                0                               1   \n",
       "1482   22       m                0                               1   \n",
       "\n",
       "     city_of_origin state_of_origin  region_of_origin  country_of_origin  \\\n",
       "0           kolkata      west bengal               NaN             india   \n",
       "1           kolkata      west bengal               NaN             india   \n",
       "2           kolkata      west bengal               NaN             india   \n",
       "3           kolkata      west bengal               NaN             india   \n",
       "4           kolkata      west bengal               NaN             india   \n",
       "...             ...              ...               ...               ...   \n",
       "1478        changde            hunan               NaN             china   \n",
       "1479        changde            hunan               NaN             china   \n",
       "1480        changde            hunan               NaN             china   \n",
       "1481        changde            hunan               NaN             china   \n",
       "1482        changde            hunan               NaN             china   \n",
       "\n",
       "      no_of_sisters  no_of_brothers  ...  \\\n",
       "0                 0               0  ...   \n",
       "1                 0               0  ...   \n",
       "2                 0               0  ...   \n",
       "3                 0               0  ...   \n",
       "4                 0               0  ...   \n",
       "...             ...             ...  ...   \n",
       "1478              0               0  ...   \n",
       "1479              0               0  ...   \n",
       "1480              0               0  ...   \n",
       "1481              0               0  ...   \n",
       "1482              0               0  ...   \n",
       "\n",
       "     dog_no_of_pedestrians_on_other_lane_died dog_no_of_passengers_saved  \\\n",
       "0                                           0                          0   \n",
       "1                                           0                          0   \n",
       "2                                           0                          0   \n",
       "3                                           0                          0   \n",
       "4                                           0                          0   \n",
       "...                                       ...                        ...   \n",
       "1478                                        0                          1   \n",
       "1479                                        0                          0   \n",
       "1480                                        0                          0   \n",
       "1481                                        0                          0   \n",
       "1482                                        0                          0   \n",
       "\n",
       "     dog_no_of_pedestrians_on_lane_ahead_saved  \\\n",
       "0                                            0   \n",
       "1                                            0   \n",
       "2                                            0   \n",
       "3                                            0   \n",
       "4                                            0   \n",
       "...                                        ...   \n",
       "1478                                         1   \n",
       "1479                                         0   \n",
       "1480                                         0   \n",
       "1481                                         0   \n",
       "1482                                         0   \n",
       "\n",
       "     dog_no_of_pedestrians_on_other_lane_saved  cat_no_of_passengers_died  \\\n",
       "0                                            0                          0   \n",
       "1                                            0                          0   \n",
       "2                                            0                          0   \n",
       "3                                            0                          0   \n",
       "4                                            0                          0   \n",
       "...                                        ...                        ...   \n",
       "1478                                         0                          0   \n",
       "1479                                         0                          0   \n",
       "1480                                         0                          0   \n",
       "1481                                         0                          0   \n",
       "1482                                         0                          0   \n",
       "\n",
       "      cat_no_of_pedestrians_on_lane_ahead_died  \\\n",
       "0                                            0   \n",
       "1                                            0   \n",
       "2                                            0   \n",
       "3                                            1   \n",
       "4                                            0   \n",
       "...                                        ...   \n",
       "1478                                         0   \n",
       "1479                                         0   \n",
       "1480                                         0   \n",
       "1481                                         0   \n",
       "1482                                         0   \n",
       "\n",
       "      cat_no_of_pedestrians_on_other_lane_died  cat_no_of_passengers_saved  \\\n",
       "0                                            0                           0   \n",
       "1                                            0                           0   \n",
       "2                                            0                           0   \n",
       "3                                            0                           0   \n",
       "4                                            0                           0   \n",
       "...                                        ...                         ...   \n",
       "1478                                         0                           0   \n",
       "1479                                         0                           0   \n",
       "1480                                         0                           0   \n",
       "1481                                         0                           0   \n",
       "1482                                         0                           0   \n",
       "\n",
       "      cat_no_of_pedestrians_on_lane_ahead_saved  \\\n",
       "0                                             0   \n",
       "1                                             0   \n",
       "2                                             0   \n",
       "3                                             0   \n",
       "4                                             0   \n",
       "...                                         ...   \n",
       "1478                                          0   \n",
       "1479                                          0   \n",
       "1480                                          0   \n",
       "1481                                          0   \n",
       "1482                                          0   \n",
       "\n",
       "      cat_no_of_pedestrians_on_other_lane_saved  \n",
       "0                                             0  \n",
       "1                                             0  \n",
       "2                                             0  \n",
       "3                                             0  \n",
       "4                                             0  \n",
       "...                                         ...  \n",
       "1478                                          0  \n",
       "1479                                          0  \n",
       "1480                                          0  \n",
       "1481                                          0  \n",
       "1482                                          0  \n",
       "\n",
       "[1483 rows x 148 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moral_df = pd.read_excel('moral_machine_data.xlsx')\n",
    "moral_df.info()\n",
    "\n",
    "#rename columns to clean them up\n",
    "col_list = [col for col in moral_df.columns]\n",
    "col_list_renamed = [col.strip() for col in col_list]\n",
    "\n",
    "renamer = dict(zip(col_list, col_list_renamed))\n",
    "moral_df.rename(mapper=renamer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "1. [10 points] Code the PCA algorithm from scratch. (Note: Your code should be able to\n",
    "process any m x n dataset). (Note: Set random.seed(265) before you start).\n",
    "\n",
    "2. [10 points] Test your algorithm on the columns that denote ‘the results of your decision’\n",
    "(more information can be found in the data dictionary above). (Note: Set number of\n",
    "dimensions to 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(265)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize data\n",
    "def standardize(mat):\n",
    "    mean = np.mean(mat, axis=0)\n",
    "    std = np.std(mat, axis=0)\n",
    "    return (mat - mean)/std\n",
    "    \n",
    "def covariance(mat):\n",
    "    return np.cov(mat.T)\n",
    "\n",
    "# find eigenvectors and eigenvalues of covariance matrix\n",
    "# sort eigenvectors acc to magnitude\n",
    "# choose top k and create transformation matrix\n",
    "def extract_principal_components(mat, k=2):\n",
    "    vals, vecs = eig(mat)\n",
    "    abs_vec = np.abs(vecs)\n",
    "    #making largest eigenvector positive directioned (for reference)\n",
    "    vecs = (vecs*np.sign(vecs[np.argmax(abs_vec, axis=0), :])).T\n",
    "    pairs = [(vals[i], vecs[i]) for i in range(len(vals))]\n",
    "    pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "    top_k_vecs = np.array([vecs[i] for i in range(len(vecs))])[:k, :]\n",
    "    return top_k_vecs\n",
    "\n",
    "# transform matrix to one in k dimensions\n",
    "def transform(top_k_vecs, og_mat):\n",
    "    return og_mat.dot(top_k_vecs.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply on the results columns (ending in _saved or _died)\n",
    "result_cols = [col for col in moral_df.columns if col.endswith('_saved') or col.endswith('_died')]\n",
    "\n",
    "matrix = moral_df[result_cols]\n",
    "matrix = standardize(matrix)\n",
    "covar_matrix = covariance(matrix)\n",
    "transformation_mat = extract_principal_components(covar_matrix)\n",
    "new_matrix = transform(transformation_mat, matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.542107</td>\n",
       "      <td>-1.419617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.009811</td>\n",
       "      <td>0.320212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.448330</td>\n",
       "      <td>-0.586891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.117906</td>\n",
       "      <td>0.181712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.393012</td>\n",
       "      <td>-0.353074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1478</th>\n",
       "      <td>1.976357</td>\n",
       "      <td>0.832954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1479</th>\n",
       "      <td>-0.174411</td>\n",
       "      <td>2.006324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1480</th>\n",
       "      <td>1.475993</td>\n",
       "      <td>3.301404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>-0.107228</td>\n",
       "      <td>2.451157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>0.709623</td>\n",
       "      <td>-0.002098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1483 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1\n",
       "0    -0.542107 -1.419617\n",
       "1     0.009811  0.320212\n",
       "2     0.448330 -0.586891\n",
       "3    -0.117906  0.181712\n",
       "4     0.393012 -0.353074\n",
       "...        ...       ...\n",
       "1478  1.976357  0.832954\n",
       "1479 -0.174411  2.006324\n",
       "1480  1.475993  3.301404\n",
       "1481 -0.107228  2.451157\n",
       "1482  0.709623 -0.002098\n",
       "\n",
       "[1483 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_matrix #with new components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the two principal components you obtained in Q1, create five scatterplots\n",
    "for the following five columns by using the visualization_code.py file in the assignment folder.\n",
    "Cluster labels will be determined by the unique values in columns of the dataset (you don’t\n",
    "need to run a separate clustering algorithm, but you will need to create class labels for some\n",
    "of the observations in the columns below) (example: USA = Cluster 0, India = Cluster 1 etc.):\n",
    "1. [3 points] age\n",
    "2. [3 points] gender\n",
    "3. [3 points] grown_up_in_US\n",
    "4. [3 points] country_of_origin\n",
    "5. [3 points] no_of_siblings (you will need to create a new column by doing: no_of_sisters + no_of_brothers)\n",
    "6. [5 points] Interpret the visuals you obtained above in around 200 words. Specifically: Do\n",
    "you see any patterns? Which column do you think creates the best clustering pattern?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO : Aabha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "Repeat Q2, this time using spectral embedding for dimensionality reduction.\n",
    "Please answer the following:\n",
    "1. [10 points] What is spectral embedding? Please do some online research and explain how\n",
    "spectral embedding works in around 200 words.\n",
    "2. [10 points] Using the SpectralEmbedding module of sklearn2, create the same\n",
    "set of five graphs (1 point each). Interpret the results in around 150 words (5 points).\n",
    "Specifically: Do you see any patterns? Which column do you think creates the best\n",
    "clustering pattern? And: Are the results better than PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Spectral Embedding is a non linear dimensionality reduction technique that preserves local relationships, especially those used in graph structures or for clustering. Unlike PCA which decomposes the covariance matrix, spectral embedding performs eigendecomposition on a Laplacian matrix that  preserves the relationship between data points. The resulting eigenvectors represent the relationships in a lower dimensional space\n",
    "\n",
    "    The process begins by representing the dataset as a similarity graph, where each data point is a node, and edges between nodes indicate similarity based on a chosen metric. The strength of these connections is quantified in an adjacency matrix. From this, the degree matrix is computed, containing the sum of connections or the degree of the nodes The graph Laplacian is then derived by subtracting the adjacency matrix from the degree matrix. This Laplacian matrix encapsulates the structure of the data.​\n",
    " \n",
    "    By performing eigendecomposition on the Laplacian matrix, we obtain its eigenvalues and eigenvectors. Unlike PCA which looks at the k largest eigenvectors, this method finds k smallest non-zero eigenvalues. This is low magnitude eigenvalues represent slowly varying eigenvectors, which capture global structure and broad patterns in the data. Conversely, large eigenvalues capture diversity and high frequency changes in the dataset.\n",
    " \n",
    "    Spectral embedding is particularly effective for clustering tasks, especially when dealing with data that exhibits complex, non-convex patterns. It has applications in various fields, including image segmentation, social network analysis, and bioinformatics, where understanding the underlying manifold structure of the data is crucial. ​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix X of dim (1483, 120) -> Transformed Matrix of dim (1483, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import SpectralEmbedding\n",
    "X = moral_df[result_cols]\n",
    "embedding = SpectralEmbedding(n_components=2)\n",
    "X_transformed = embedding.fit_transform(X)\n",
    "print(f\"Matrix X of dim {X.shape} -> Transformed Matrix of dim {X_transformed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: @Aabha : can u recreate the graphs for q1 here too!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "[20 points] Repeat Q2, this time using T-SNE for dimensionality reduction. Please answer the\n",
    "following:\n",
    "1. [10 points] What is T-SNE? Please do some online research and explain how T-SNE works\n",
    "in around 200 words.\n",
    "2. [10 points] Using the TSNE module of sklearn3, create the same set of five graphs (1\n",
    "point each). Interpret the results 150 words (5 points). Specifically: Do you see any\n",
    "patterns? Which column do you think creates the best clustering pattern? And: Compare\n",
    "the results to PCA and Spectral Embedding. Are the results any better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. T-SNE (t-Distributed Stochastic Neighbor Embedding) is a non-linear dimensionality reduction technique that uses probabilistic pairwise similarity between data points in both high-dimensional and low-dimensional spaces to ensure that similar points remain close together in the embedding.        \n",
    "\n",
    "    The process begins by calculating pairwise similarities between data points using a Gaussian distribution based on Euclidean distances. Points that are closer in high-dimensional space have higher similarity scores.\n",
    "\n",
    "    Next, T-SNE randomly initializes a set of points in the lower-dimensional space (usually 2D or 3D) and computes pairwise similarities between them using a Student’s t-distribution. The t-distribution’s heavy tails allow for better separation of distant points, avoiding crowding in the lower dimensions.\n",
    "\n",
    "    To align the high-dimensional similarities with the low-dimensional ones, the algorithm minimizes the Kullback-Leibler (KL) divergence, a measure of dissimilarity between the two distributions, using gradient descent.\n",
    "\n",
    "    T-SNE is highly effective for visualizing complex data, often revealing clusters and patterns. However, it is computationally expensive and does not preserve global structure as well as local relationships, making it more suitable for data exploration rather than precise metric-based analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix X of dim (1483, 120) -> Transformed Matrix of dim (1483, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "X_tsne = TSNE(n_components=2, \n",
    "                  learning_rate='auto',\n",
    "                  init='random', \n",
    "                  perplexity=3).fit_transform(X)\n",
    "print(f\"Matrix X of dim {X.shape} -> Transformed Matrix of dim {X_tsne.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: @Aabha: Can you do the visuals?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, create a correlation matrix4 that looks at the correlations between all of\n",
    "the numerical and numerically coded variables in your dataset (excluding the string variables) and also the six new dimension reduction columns you created in Q2, Q3, and Q4. Please do\n",
    "the following:\n",
    "1. [5 points] Create the correlation matrix.\n",
    "2. [5 points] Interpret the results in around 250 words. Specifically, answer the\n",
    "following: Are there any variables that are strongly correlated? (i) Are there any\n",
    "variables from the original dataset that are strongly correlated with the dimension\n",
    "reduction variables? (ii) Do you think there are any variables in the original dataset\n",
    "that are strongly represented by the dimension reduction variables? (iii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['spectral_c1', 'spectral_c2', 'tsne_c1', 'tsne_c2', 'pca_c1', 'pca_c2'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "numeric_cols = moral_df.select_dtypes(include=np.number)\n",
    "\n",
    "#adding our embeddings\n",
    "numeric_cols['spectral_c1'] = X_transformed.T[0]\n",
    "numeric_cols['spectral_c2'] = X_transformed.T[1]\n",
    "\n",
    "numeric_cols['tsne_c1'] = X_tsne.T[0]\n",
    "numeric_cols['tsne_c2'] = X_tsne.T[1]\n",
    "\n",
    "numeric_cols['pca_c1'] = new_matrix[0]\n",
    "numeric_cols['pca_c2'] = new_matrix[1]\n",
    "\n",
    "print(numeric_cols.columns[-6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pedestrians_ahead_vs_passengers           pedestrians_on_other_lane_crossing   -0.963335\n",
      "passengers_in_the_self_driving_car        pedestrians_vs_pedestrians           -0.960889\n",
      "passengers_vs_pedestrians_on_other_lane   pedestrians_on_lane_crossing_ahead   -0.943989\n",
      "cat_no_of_pedestrians_on_lane_ahead_died  spectral_c2                           0.790923\n",
      "dog_no_of_pedestrians_on_lane_ahead_died  spectral_c2                           0.762263\n",
      "spectral_c1                               tsne_c1                               0.728717\n",
      "pedestrians_on_lane_ahead_should_die      spectral_c1                           0.682536\n",
      "pedestrians_ahead_vs_passengers           pedestrians_vs_pedestrians           -0.653102\n",
      "pedestrians_on_other_lane_crossing        pedestrians_vs_pedestrians            0.634352\n",
      "passengers_in_the_self_driving_car        pedestrians_ahead_vs_passengers       0.625799\n",
      "                                          pedestrians_on_other_lane_crossing   -0.613140\n",
      "pedestrians_on_lane_ahead_should_die      tsne_c1                               0.609631\n",
      "illegal_crossing_ahead                    legal_crossing_ahead                 -0.587021\n",
      "                                          traffic_lights                        0.566835\n",
      "passengers_in_the_car_should_die          passengers_in_the_self_driving_car    0.565733\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "corr_matrix = numeric_cols.corr()\n",
    "\n",
    "\n",
    "corr_pairs = corr_matrix.unstack()\n",
    "corr_pairs = corr_pairs[corr_pairs != 1]\n",
    "\n",
    "\n",
    "# drop duplicate pairs \n",
    "corr_pairs = corr_pairs.sort_index()\n",
    "corr_pairs = corr_pairs[corr_pairs.index.get_level_values(0) < corr_pairs.index.get_level_values(1)]\n",
    "\n",
    "\n",
    "threshold = 0.5\n",
    "significant_corrs = corr_pairs[abs(corr_pairs) > threshold]\n",
    "significant_corrs = significant_corrs.sort_values(ascending=False, key=lambda x:np.abs(x))\n",
    "\n",
    "# top N significant unique pairs\n",
    "top_n = significant_corrs.head(15)  \n",
    "print(top_n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-res",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

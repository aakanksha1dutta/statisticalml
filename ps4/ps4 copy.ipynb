{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 4 \n",
    "\n",
    "\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import eig\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1483 entries, 0 to 1482\n",
      "Columns: 148 entries, age to cat_no_of_pedestrians_on_other_lane_saved\n",
      "dtypes: int64(139), object(9)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "moral_df = pd.read_excel('moral_machine_data.xlsx')\n",
    "moral_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "[10 points] To develop a better understanding of the Moral Machine Experiment, please read the academic article on ‘Moral Machine Experiment’. Provide a summary of around 250 words by talking about the goals of the paper, data, and the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The **Moral Machine experiment** explores moral preferences of the public regarding ethical dilemmas faced by autonomous vehicles (AVs). This comes at a time when machines are being integrated into more and more human tasks and are faced with potential situations where human harm cannot be eliminated. Researchers developed the Moral Machine Experiment to get a more centralized view of such moral situations, which can then be taught to machines. This is an online platform that presents participants with various moral dilemmas, such as sacrificing passengers versus pedestrians or deciding whom to prioritize among potential victims. (in terms of traffic scenarios.) where AVs must choose between two different outcomes.\n",
    "\n",
    "The data collected (which is all public) was first used to summerhouse global preferences. 9 categories of preference were explored. In general, participants showed a tendency to favor human lives over animals, prioritizing more lives over fewer lives, and saving younger individuals over older ones. However, researchers determined it would also be beneficial to be more specific with the preferences. 492, 921 respondents also completed a survey on age, education, gender, income, and political and religious views. This second analysis was done to observe the individual variation in choices. The researchers then looked at cultural clusters, identifying 3 clusters with distinct moral preferences: Western, Eastern, and Southern.  These variations in moral preferences also correlated with countries' cultural values, economic conditions, and institutions. For example, places with high economic inequality appeared to influence decisions about prioritizing individuals of higher or lower social status."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "1. [10 points] Code the PCA algorithm from scratch. (Note: Your code should be able to\n",
    "process any m x n dataset). (Note: Set random.seed(265) before you start).\n",
    "\n",
    "2. [10 points] Test your algorithm on the columns that denote ‘the results of your decision’\n",
    "(more information can be found in the data dictionary above). (Note: Set number of\n",
    "dimensions to 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(265)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize data\n",
    "def standardize(mat):\n",
    "    mean = np.mean(mat, axis=0)\n",
    "    std = np.std(mat, axis=0)\n",
    "    return (mat - mean)/std\n",
    "    \n",
    "def covariance(mat):\n",
    "    return np.cov(mat.T)\n",
    "\n",
    "# find eigenvectors and eigenvalues of covariance matrix\n",
    "# sort eigenvectors acc to magnitude\n",
    "# choose top k and create transformation matrix\n",
    "def extract_principal_components(mat, k=2):\n",
    "    vals, vecs = eig(mat)\n",
    "    abs_vec = np.abs(vecs)\n",
    "    #making largest eigenvector positive directioned (for reference)\n",
    "    vecs = (vecs*np.sign(vecs[np.argmax(abs_vec, axis=0), :])).T\n",
    "    pairs = [(vals[i], vecs[i]) for i in range(len(vals))]\n",
    "    pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "    top_k_vecs = np.array([vecs[i] for i in range(len(vecs))])[:k, :]\n",
    "    return top_k_vecs\n",
    "\n",
    "# transform matrix to one in k dimensions\n",
    "def transform(top_k_vecs, og_mat):\n",
    "    return og_mat.dot(top_k_vecs.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply on the results columns (ending in _saved or _died)\n",
    "result_cols = [col for col in moral_df.columns if col.endswith('_saved') or col.endswith('_died')]\n",
    "\n",
    "matrix = moral_df[result_cols]\n",
    "matrix = standardize(matrix)\n",
    "covar_matrix = covariance(matrix)\n",
    "transformation_mat = extract_principal_components(covar_matrix)\n",
    "new_matrix = transform(transformation_mat, matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.229410</td>\n",
       "      <td>-0.251343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.182999</td>\n",
       "      <td>0.340581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.155791</td>\n",
       "      <td>-0.566522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.091939</td>\n",
       "      <td>0.202081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.602857</td>\n",
       "      <td>0.680327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1478</th>\n",
       "      <td>1.348812</td>\n",
       "      <td>-0.276964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1479</th>\n",
       "      <td>-0.001224</td>\n",
       "      <td>2.026693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1480</th>\n",
       "      <td>-0.746230</td>\n",
       "      <td>3.321773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>0.010015</td>\n",
       "      <td>1.726834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>0.585775</td>\n",
       "      <td>2.059617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1483 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1\n",
       "0    -0.229410 -0.251343\n",
       "1     0.182999  0.340581\n",
       "2    -0.155791 -0.566522\n",
       "3    -0.091939  0.202081\n",
       "4    -0.602857  0.680327\n",
       "...        ...       ...\n",
       "1478  1.348812 -0.276964\n",
       "1479 -0.001224  2.026693\n",
       "1480 -0.746230  3.321773\n",
       "1481  0.010015  1.726834\n",
       "1482  0.585775  2.059617\n",
       "\n",
       "[1483 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_matrix #with new components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the two principal components you obtained in Q1, create five scatterplots\n",
    "for the following five columns by using the visualization_code.py file in the assignment folder.\n",
    "Cluster labels will be determined by the unique values in columns of the dataset (you don’t\n",
    "need to run a separate clustering algorithm, but you will need to create class labels for some\n",
    "of the observations in the columns below) (example: USA = Cluster 0, India = Cluster 1 etc.):\n",
    "1. [3 points] age\n",
    "2. [3 points] gender\n",
    "3. [3 points] grown_up_in_US\n",
    "4. [3 points] country_of_origin\n",
    "5. [3 points] no_of_siblings (you will need to create a new column by doing: no_of_sisters + no_of_brothers)\n",
    "6. [5 points] Interpret the visuals you obtained above in around 200 words. Specifically: Do\n",
    "you see any patterns? Which column do you think creates the best clustering pattern?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "Repeat Q2, this time using spectral embedding for dimensionality reduction.\n",
    "Please answer the following:\n",
    "1. [10 points] What is spectral embedding? Please do some online research and explain how\n",
    "spectral embedding works in around 200 words.\n",
    "2. [10 points] Using the SpectralEmbedding module of sklearn2, create the same\n",
    "set of five graphs (1 point each). Interpret the results in around 150 words (5 points).\n",
    "Specifically: Do you see any patterns? Which column do you think creates the best\n",
    "clustering pattern? And: Are the results better than PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Spectral Embedding is a non linear dimensionality reduction technique that preserves local relationships, especially those used in graph structures or for clustering. Unlike PCA which decomposes the covariance matrix, spectral embedding performs eigendecomposition on a Laplacian matrix that  preserves the relationship between data points. The resulting eigenvectors represent the relationships in a lower dimensional space\n",
    "\n",
    "    The process begins by representing the dataset as a similarity graph, where each data point is a node, and edges between nodes indicate similarity based on a chosen metric. The strength of these connections is quantified in an adjacency matrix. From this, the degree matrix is computed, containing the sum of connections or the degree of the nodes The graph Laplacian is then derived by subtracting the adjacency matrix from the degree matrix. This Laplacian matrix encapsulates the structure of the data.​\n",
    " \n",
    "    By performing eigendecomposition on the Laplacian matrix, we obtain its eigenvalues and eigenvectors. Unlike PCA which looks at the k largest eigenvectors, this method finds k smallest non-zero eigenvalues. This is low magnitude eigenvalues represent slowly varying eigenvectors, which capture global structure and broad patterns in the data. Conversely, large eigenvalues capture diversity and high frequency changes in the dataset.\n",
    " \n",
    "    Spectral embedding is particularly effective for clustering tasks, especially when dealing with data that exhibits complex, non-convex patterns. It has applications in various fields, including image segmentation, social network analysis, and bioinformatics, where understanding the underlying manifold structure of the data is crucial. ​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix X of dim (1483, 120) -> Transformed Matrix of dim (1483, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import SpectralEmbedding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X = moral_df[result_cols]\n",
    "embedding = SpectralEmbedding(n_components=2, random_state=265)\n",
    "X_transformed = embedding.fit_transform(X)\n",
    "print(f\"Matrix X of dim {X.shape} -> Transformed Matrix of dim {X_transformed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import interpolate\n",
    "from scipy.spatial import ConvexHull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'gender'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'gender'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m moral_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_dim_2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m X_transformed[:,\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      5\u001b[0m moral_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno_of_sublings\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m moral_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno_of_brothers\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m moral_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno_of_sisters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 7\u001b[0m moral_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgender_label\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m moral_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgender\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m      8\u001b[0m moral_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrown_up_in_US_label\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrown_up_in_US\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m      9\u001b[0m moral_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcountry_of_origin_label\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mfactorize(moral_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcountry_of_origin\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'gender'"
     ]
    }
   ],
   "source": [
    "#TO-DO\n",
    "moral_df['X_dim_1'] = X_transformed[:,0]\n",
    "moral_df['X_dim_2'] = X_transformed[:,1]\n",
    "\n",
    "moral_df['no_of_sublings'] = moral_df['no_of_brothers'] + moral_df['no_of_sisters']\n",
    "\n",
    "moral_df['gender_label'] = moral_df['gender'].astype(str)\n",
    "moral_df['grown_up_in_US_label'] = df['grown_up_in_US'].astype(str)\n",
    "moral_df['country_of_origin_label'] = pd.factorize(moral_df['country_of_origin'])\n",
    "moral_df['no_of_siblings_label'] = moral_df['no_of_siblings']\n",
    "\n",
    "data = {\n",
    "    'age':'Age',\n",
    "    'gender_label': 'Gender',\n",
    "    'grown_up_in_US_label': 'Country of Origin',\n",
    "    'no_of_siblings_label':'Number of Siblings'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.dpi'] = 600\n",
    "plt.rcParams['figure.figsize'] = (20.0, 10.0)\n",
    "plt.rcParams['font.family'] = \"serif\"\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "for col, title in data.items():\n",
    "    moral_df['kmeans'] = mpl.rcParams['figure.dpi'] = 600\n",
    "plt.rcParams['figure.figsize'] = (20.0, 10.0)\n",
    "plt.rcParams['font.family'] = \"serif\"\n",
    "plt.rcParams['font.size'] = 10[col]\n",
    "\n",
    "pal = sns.color_palette(\"Paired\", n_colors=len(set(moral_df['kmeans_label'])))\n",
    "\n",
    "label_values = moral_df['kmeans_label'].unique()\n",
    "label_to_color_index = {label: idx for idx, label in enumerate(label_values)}\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "\n",
    "pl = sns.scatterplot(\n",
    "    x=\"x_dim_1\", y=\"x_dim_2\",\n",
    "    hue=\"kmeans_label\", palette=pal, data=moral_df,\n",
    "    s=250, alpha=0.7, legend=False\n",
    ")\n",
    "\n",
    "for line in range(moral_df.sjhape[0]):\n",
    "    pl.text(\n",
    "        moral_df.x_dim_1[line], moral_df.x_dim_2[line],\n",
    "        str(moral_df.kmeans_label[line]),\n",
    "        horizontalalignment='left', size='medium',\n",
    "        color='black', weight='semibold'\n",
    "    )\n",
    "\n",
    "plt.suptitle(f\"Spectral Embedding Scatter Plot by {title}\", fontsize=36)\n",
    "plt.xlable(\"Spectral Dim 1\", font_size=24)\n",
    "plt.ylable(\"Spectral Dim 2\", font_size=24)\n",
    "\n",
    "for label in label_values:\n",
    "\n",
    "    points = moral_df[moral_df.kmeans == label][['x_dim_1', 'x_dim_2']].drop_duplicates()\n",
    "    if len(points) <3:\n",
    "        continue\n",
    "    hull = ConvexHull(points)\n",
    "\n",
    "    x_hull = np.append(points[hull.vertivces, 0], points[hull.vertices, 0][0])\n",
    "    y_hull = np.append(points[hull.vertivces, 1], points[hull.vertices, 1][0])\n",
    "\n",
    "    #interplotate\n",
    "    dist = np.sqrt((x_hull[:-1] - x_hull[1:]) ** 2 + (y_hull[:-1] - y_hull[1:]) ** 2)\n",
    "    dist_along = np.concatenate(([0], dist.cumsum()))\n",
    "    spline, _ = interpolate.splprep([x_hull, y_hull], u=dist_along, s=0)\n",
    "    interp_d = np.linspace(dist_along[0], dist_along[-1], 50)\n",
    "    interp_x, interp_y = interpolate.splev(interp_d, spline)\n",
    "\n",
    "    plt.fill(interp_x, interp_y, '--', c=pal[label_to_color_index[label]], alpha=0.2)\n",
    "\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "[20 points] Repeat Q2, this time using T-SNE for dimensionality reduction. Please answer the\n",
    "following:\n",
    "1. [10 points] What is T-SNE? Please do some online research and explain how T-SNE works\n",
    "in around 200 words.\n",
    "2. [10 points] Using the TSNE module of sklearn3, create the same set of five graphs (1\n",
    "point each). Interpret the results 150 words (5 points). Specifically: Do you see any\n",
    "patterns? Which column do you think creates the best clustering pattern? And: Compare\n",
    "the results to PCA and Spectral Embedding. Are the results any better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. T-SNE (t-Distributed Stochastic Neighbor Embedding) is a non-linear dimensionality reduction technique that uses probabilistic pairwise similarity between data points in both high-dimensional and low-dimensional spaces to ensure that similar points remain close together in the embedding.        \n",
    "\n",
    "    The process begins by calculating pairwise similarities between data points using a Gaussian distribution based on Euclidean distances. Points that are closer in high-dimensional space have higher similarity scores.\n",
    "\n",
    "    Next, T-SNE randomly initializes a set of points in the lower-dimensional space (usually 2D or 3D) and computes pairwise similarities between them using a Student’s t-distribution. The t-distribution’s heavy tails allow for better separation of distant points, avoiding crowding in the lower dimensions.\n",
    "\n",
    "    To align the high-dimensional similarities with the low-dimensional ones, the algorithm minimizes the Kullback-Leibler (KL) divergence, a measure of dissimilarity between the two distributions, using gradient descent.\n",
    "\n",
    "    T-SNE is highly effective for visualizing complex data, often revealing clusters and patterns. However, it is computationally expensive and does not preserve global structure as well as local relationships, making it more suitable for data exploration rather than precise metric-based analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix X of dim (1483, 120) -> Transformed Matrix of dim (1483, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "X_tsne = TSNE(n_components=2, \n",
    "                  learning_rate='auto',\n",
    "                  init='random', \n",
    "                  perplexity=3).fit_transform(X)\n",
    "print(f\"Matrix X of dim {X.shape} -> Transformed Matrix of dim {X_tsne.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, create a correlation matrix4 that looks at the correlations between all of\n",
    "the numerical and numerically coded variables in your dataset (excluding the string variables) and also the six new dimension reduction columns you created in Q2, Q3, and Q4. Please do\n",
    "the following:\n",
    "1. [5 points] Create the correlation matrix.\n",
    "2. [5 points] Interpret the results in around 250 words. Specifically, answer the\n",
    "following: Are there any variables that are strongly correlated? (i) Are there any\n",
    "variables from the original dataset that are strongly correlated with the dimension\n",
    "reduction variables? (ii) Do you think there are any variables in the original dataset\n",
    "that are strongly represented by the dimension reduction variables? (iii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['spectral_c1', 'spectral_c2', 'tsne_c1', 'tsne_c2', 'pca_c1', 'pca_c2'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "numeric_cols = moral_df.select_dtypes(include=np.number)\n",
    "\n",
    "#adding our embeddings\n",
    "numeric_cols['spectral_c1'] = X_transformed.T[0]\n",
    "numeric_cols['spectral_c2'] = X_transformed.T[1]\n",
    "\n",
    "numeric_cols['tsne_c1'] = X_tsne.T[0]\n",
    "numeric_cols['tsne_c2'] = X_tsne.T[1]\n",
    "\n",
    "numeric_cols['pca_c1'] = new_matrix[0]\n",
    "numeric_cols['pca_c2'] = new_matrix[1]\n",
    "\n",
    "print(numeric_cols.columns[-6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pedestrians_ahead_vs_passengers           pedestrians_on_other_lane_crossing   -0.963335\n",
      "passengers_in_the_self_driving_car        pedestrians_vs_pedestrians           -0.960889\n",
      "passengers_vs_pedestrians_on_other_lane   pedestrians_on_lane_crossing_ahead   -0.943989\n",
      "cat_no_of_pedestrians_on_lane_ahead_died  spectral_c2                           0.790923\n",
      "dog_no_of_pedestrians_on_lane_ahead_died  spectral_c2                           0.762263\n",
      "spectral_c1                               tsne_c1                               0.728717\n",
      "pedestrians_on_lane_ahead_should_die      spectral_c1                           0.682536\n",
      "pedestrians_ahead_vs_passengers           pedestrians_vs_pedestrians           -0.653102\n",
      "pedestrians_on_other_lane_crossing        pedestrians_vs_pedestrians            0.634352\n",
      "passengers_in_the_self_driving_car        pedestrians_ahead_vs_passengers       0.625799\n",
      "                                          pedestrians_on_other_lane_crossing   -0.613140\n",
      "pedestrians_on_lane_ahead_should_die      tsne_c1                               0.609631\n",
      "illegal_crossing_ahead                    legal_crossing_ahead                 -0.587021\n",
      "                                          traffic_lights                        0.566835\n",
      "passengers_in_the_car_should_die          passengers_in_the_self_driving_car    0.565733\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "corr_matrix = numeric_cols.corr()\n",
    "\n",
    "\n",
    "corr_pairs = corr_matrix.unstack()\n",
    "corr_pairs = corr_pairs[corr_pairs != 1]\n",
    "\n",
    "\n",
    "# drop duplicate pairs \n",
    "corr_pairs = corr_pairs.sort_index()\n",
    "corr_pairs = corr_pairs[corr_pairs.index.get_level_values(0) < corr_pairs.index.get_level_values(1)]\n",
    "\n",
    "\n",
    "threshold = 0.5\n",
    "significant_corrs = corr_pairs[abs(corr_pairs) > threshold]\n",
    "significant_corrs = significant_corrs.sort_values(ascending=False, key=lambda x:np.abs(x))\n",
    "\n",
    "# top N significant unique pairs\n",
    "top_n = significant_corrs.head(15)  \n",
    "print(top_n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

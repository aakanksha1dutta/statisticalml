{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 2\n",
    "## Aakanksha Dutta & Aabha Pandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# q3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_california_housing()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "Y = data.target\n",
    "\n",
    "\n",
    "features = [\"MedInc\", \"HouseAge\", \"AveRooms\", \"AveBedrms\", \"Population\", \"AveOccup\", \"Latitude\", \"Longitude\"]\n",
    "X = X[features]\n",
    "\n",
    "# 0-1 normalization\n",
    "X = (X - X.min()) / (X.max() - X.min())\n",
    "Y = (Y - Y.min()) / (Y.max() - Y.min())\n",
    "\n",
    "\n",
    "X = X.to_numpy()\n",
    "Y = Y.reshape(-1, 1)\n",
    "\n",
    "# bias terms\n",
    "X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=265)\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(X, Y, learning_rate=0.01, epochs=1000):\n",
    "    m, n = X.shape\n",
    "    weights = np.zeros((n, 1))  # Initialize weights\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(m):\n",
    "            idx = np.random.randint(0, m)  \n",
    "            x_i = X[idx].reshape(1, -1)\n",
    "            y_i = Y[idx].reshape(1, -1)\n",
    "            \n",
    "            prediction = np.dot(x_i, weights)\n",
    "            error = prediction - y_i\n",
    "            \n",
    "            # Update weights using SGD update rule\n",
    "            weights -= learning_rate * x_i.T @ error\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.02204339920941451\n",
      "Testing MSE: 0.023050721944464954\n",
      "Learned Weights: [ 0.73996337  1.30962198  0.10408374 -3.01055051  4.72020398 -0.06997644\n",
      " -0.82964972 -0.82005066 -0.91684462]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "weights = stochastic_gradient_descent(X_train, Y_train, learning_rate, epochs)\n",
    "\n",
    "# Predict function\n",
    "def predict(X, weights):\n",
    "    return np.dot(X, weights)\n",
    "\n",
    "# Compute Mean Squared Error (MSE)\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Evaluate the model\n",
    "y_train_pred = predict(X_train, weights)\n",
    "y_test_pred = predict(X_test, weights)\n",
    "\n",
    "mse_train = mse(Y_train, y_train_pred)\n",
    "mse_test = mse(Y_test, y_test_pred)\n",
    "\n",
    "print(f\"Training MSE: {mse_train}\")\n",
    "print(f\"Testing MSE: {mse_test}\")\n",
    "print(f\"Learned Weights: {weights.ravel()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important Factors:\n",
    "* population (+4.7874) → strong positive effect.\n",
    "* average bedrooms per dwelling (-2.9529) → more bedrooms seem to lower house prices, likely due to smaller rooms.\n",
    "* house age (+1.3161) → older houses have higher prices, possibly due to location.\n",
    "* median income (+0.7355) → wealthier areas have more expensive homes.\n",
    "* latitude & longitude (-0.83, -0.82) → location matters significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_training mse_ = 0.0220, _testing mse_ = 0.0231\n",
    "* since the training and testing mse values are very close, this means our model generalizes well.\n",
    "* the small mse values suggest that our model is making relatively accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# q4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SGDRegressor Results:\n",
      "Training MSE: 0.022833223249611876\n",
      "Testing MSE: 0.023289385971676832\n",
      "Learned Weights: [ 0.25506976  0.23673588  0.23673588  1.13991477  0.11819594  0.07999486\n",
      "  0.15911785 -0.01362413 -0.07618674 -0.77718429 -0.85884423]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=265)\n",
    "X_train = np.c_[np.ones(X_train.shape[0]), X_train]\n",
    "X_test = np.c_[np.ones(X_test.shape[0]), X_test]\n",
    "\n",
    "# Hyperparameters \n",
    "learning_rate = 0.01\n",
    "num_epochs = 1000\n",
    "n_samples, n_features = X_train.shape \n",
    "weights = np.random.randn(n_features, 1)\n",
    "\n",
    "# Using Scikit-Learn's SGDRegressor\n",
    "sgd_reg = SGDRegressor(loss='squared_error', learning_rate='constant', eta0=learning_rate, max_iter=num_epochs, random_state=265)\n",
    "sgd_reg.fit(X_train, y_train.ravel())\n",
    "\n",
    "# Compute MSE\n",
    "train_mse_sklearn = np.mean((sgd_reg.predict(X_train) - y_train.ravel()) ** 2)\n",
    "test_mse_sklearn = np.mean((sgd_reg.predict(X_test) - y_test.ravel()) ** 2)\n",
    "\n",
    "print(\"\\nSGDRegressor Results:\")\n",
    "print(f\"Training MSE: {train_mse_sklearn}\")\n",
    "print(f\"Testing MSE: {test_mse_sklearn}\")\n",
    "print(f\"Learned Weights: {np.hstack((sgd_reg.intercept_.reshape(-1), sgd_reg.coef_))}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the Weights:\n",
    "AveBedrms (4.787) has the strongest positive impact on house prices, meaning more bedrooms per household correlate with higher prices.\n",
    "HouseAge (1.316) also has a strong positive effect, meaning older houses might be in established neighborhoods with higher demand.\n",
    "Population (-2.952) has the strongest negative impact, indicating that densely populated areas tend to have lower house prices.\n",
    "Longitude (-0.904) and Latitude (-0.820) have negative coefficients, suggesting a geographical pricing pattern (e.g., inland locations may be less expensive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very similar result as our custom Stochastic Gradient Descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calcuate cov of each entry\n",
    "def cov(a,b):\n",
    "    a_mean = a.mean()\n",
    "    b_mean = b.mean()\n",
    "    n = len(a) #number of rows to divide by\n",
    "    return sum((a - a_mean) * (b - b_mean))/n #no need to transpose since each Mij being calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the matrix\n",
    "\n",
    "def covMatix(data):\n",
    "    #collect number of rows and cols in data\n",
    "    rows, col = data.shape\n",
    "\n",
    "    #shape is feature x feature so initilising zero matrix\n",
    "    matrix = np.zeros((col,col))\n",
    "\n",
    "    for i in range(col):\n",
    "        for j in range(col):\n",
    "            matrix[i][j] = cov(data[:,i], data[:,j])\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
      "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
      "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
      "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
      "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
      "\n",
      "   Longitude  MedHouseVal  \n",
      "0    -122.23        4.526  \n",
      "1    -122.22        3.585  \n",
      "2    -122.24        3.521  \n",
      "3    -122.25        3.413  \n",
      "4    -122.25        3.422  \n"
     ]
    }
   ],
   "source": [
    "# Load the California Housing dataset\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "housing_df = housing.frame #for .cov() to check\n",
    "data = housing_df.values\n",
    "print(housing_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance-Covariance Matrix:\n",
      "[[ 3.60914769e+00 -2.84600238e+00  1.53649356e+00 -5.58548886e-02\n",
      "   1.04004753e+01  3.70270956e-01 -3.23844062e-01 -5.77619034e-02\n",
      "   1.50840174e+00]\n",
      " [-2.84600238e+00  1.58388586e+02 -4.77265121e+00 -4.63695945e-01\n",
      "  -4.22206601e+03  1.72421442e+00  3.00330956e-01 -2.72811148e+00\n",
      "   1.53391369e+00]\n",
      " [ 1.53649356e+00 -4.77265121e+00  6.12123614e+00  9.93819648e-01\n",
      "  -2.02323909e+02 -1.24682825e-01  5.62208233e-01 -1.36511756e-01\n",
      "   4.33804618e-01]\n",
      " [-5.58548886e-02 -4.63695945e-01  9.93819648e-01  2.24580619e-01\n",
      "  -3.55255040e+01 -3.04227797e-02  7.05718662e-02  1.26698232e-02\n",
      "  -2.55379820e-02]\n",
      " [ 1.04004753e+01 -4.22206601e+03 -2.02323909e+02 -3.55255040e+01\n",
      "   1.28240832e+06  8.21672190e+02 -2.63125065e+02  2.26366871e+02\n",
      "  -3.22109266e+01]\n",
      " [ 3.70270956e-01  1.72421442e+00 -1.24682825e-01 -3.04227797e-02\n",
      "   8.21672190e+02  1.07864799e+02  5.24890984e-02  5.15162217e-02\n",
      "  -2.84480199e-01]\n",
      " [-3.23844062e-01  3.00330956e-01  5.62208233e-01  7.05718662e-02\n",
      "  -2.63125065e+02  5.24890984e-02  4.56207160e+00 -3.95686200e+00\n",
      "  -3.55308375e-01]\n",
      " [-5.77619034e-02 -2.72811148e+00 -1.36511756e-01  1.26698232e-02\n",
      "   2.26366871e+02  5.15162217e-02 -3.95686200e+00  4.01394488e+00\n",
      "  -1.06269103e-01]\n",
      " [ 1.50840174e+00  1.53391369e+00  4.33804618e-01 -2.55379820e-02\n",
      "  -3.22109266e+01 -2.84480199e-01 -3.55308375e-01 -1.06269103e-01\n",
      "   1.33155030e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Compute the covariance matrix\n",
    "cov_matrix = covMatix(data)\n",
    "\n",
    "# Print the covariance matrix\n",
    "print(\"Variance-Covariance Matrix:\")\n",
    "print(cov_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                MedInc     HouseAge    AveRooms  AveBedrms    Population  \\\n",
      "MedInc        3.609323    -2.846140    1.536568  -0.055858  1.040098e+01   \n",
      "HouseAge     -2.846140   158.396260   -4.772882  -0.463718 -4.222271e+03   \n",
      "AveRooms      1.536568    -4.772882    6.121533   0.993868 -2.023337e+02   \n",
      "AveBedrms    -0.055858    -0.463718    0.993868   0.224592 -3.552723e+01   \n",
      "Population   10.400979 -4222.270582 -202.333712 -35.527225  1.282470e+06   \n",
      "AveOccup      0.370289     1.724298   -0.124689  -0.030424  8.217120e+02   \n",
      "Latitude     -0.323860     0.300346    0.562235   0.070575 -2.631378e+02   \n",
      "Longitude    -0.057765    -2.728244   -0.136518   0.012670  2.263778e+02   \n",
      "MedHouseVal   1.508475     1.533988    0.433826  -0.025539 -3.221249e+01   \n",
      "\n",
      "               AveOccup    Latitude   Longitude  MedHouseVal  \n",
      "MedInc         0.370289   -0.323860   -0.057765     1.508475  \n",
      "HouseAge       1.724298    0.300346   -2.728244     1.533988  \n",
      "AveRooms      -0.124689    0.562235   -0.136518     0.433826  \n",
      "AveBedrms     -0.030424    0.070575    0.012670    -0.025539  \n",
      "Population   821.712002 -263.137814  226.377839   -32.212487  \n",
      "AveOccup     107.870026    0.052492    0.051519    -0.284494  \n",
      "Latitude       0.052492    4.562293   -3.957054    -0.355326  \n",
      "Longitude      0.051519   -3.957054    4.014139    -0.106274  \n",
      "MedHouseVal   -0.284494   -0.355326   -0.106274     1.331615  \n"
     ]
    }
   ],
   "source": [
    "# checking using pandas cov function\n",
    "covMatrix = housing_df.cov()\n",
    "print(covMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigmoid function\n",
    "def sigmoid_f(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hypothesis\n",
    "def classifer_f(X, theta):\n",
    "    return sigmoid_f(np.dot(X, theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#entropy\n",
    "def binary_loss(y, y_pred): #y_pred = classifer_f??\n",
    "    m = len(y)\n",
    "    return - (1 / m) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stochastic gradient descent \n",
    "def gradient_f(X, y, theta, alpha, iterations):\n",
    "    m = len(y)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        y_pred = classifer_f(X, theta)\n",
    "        gradient = np.dot(X.T, (y_pred - y)) / m\n",
    "        theta -= alpha * gradient\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer function\n",
    "\n",
    "def optimizer_f(X, y, alpha, iterations):\n",
    "    \n",
    "    X = np.c_[np.ones(X.shape[0]), X] #col of 1s for intercept (for matrix multiplication)\n",
    "    theta = np.zeros(X.shape[1]) #intialize theta vector\n",
    "    \n",
    "    # run gradient descent\n",
    "    theta_opt = gradient_f(X, y, theta, alpha, iterations)\n",
    "    \n",
    "    return theta_opt, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Logistic Regression Equation:\n",
      "9.7400*Intercept + -1.2493*mean radius + -1.9910*mean texture + -1.3578*mean perimeter + -1.8667*mean area + 0.3557*mean smoothness + -0.4573*mean compactness + -2.9788*mean concavity + -4.1637*mean concave points + 0.3137*mean symmetry + 2.6421*mean fractal dimension + -2.4517*radius error + 0.3583*texture error + -1.8514*perimeter error + -1.7106*area error + 0.3479*smoothness error + 1.5651*compactness error + 0.7751*concavity error + 0.3495*concave points error + 0.8047*symmetry error + 1.3182*fractal dimension error + -3.0011*worst radius + -3.1849*worst texture + -2.7465*worst perimeter + -2.7538*worst area + -1.6942*worst smoothness + -1.0459*worst compactness + -2.0364*worst concavity + -3.9194*worst concave points + -1.6949*worst symmetry + -0.2300*worst fractal dimension\n",
      "\n",
      "Feature Coefficients (Ranked from Positive to Negative):\n",
      "Intercept: 9.7400\n",
      "mean fractal dimension: 2.6421\n",
      "compactness error: 1.5651\n",
      "fractal dimension error: 1.3182\n",
      "symmetry error: 0.8047\n",
      "concavity error: 0.7751\n",
      "texture error: 0.3583\n",
      "mean smoothness: 0.3557\n",
      "concave points error: 0.3495\n",
      "smoothness error: 0.3479\n",
      "mean symmetry: 0.3137\n",
      "worst fractal dimension: -0.2300\n",
      "mean compactness: -0.4573\n",
      "worst compactness: -1.0459\n",
      "mean radius: -1.2493\n",
      "mean perimeter: -1.3578\n",
      "worst smoothness: -1.6942\n",
      "worst symmetry: -1.6949\n",
      "area error: -1.7106\n",
      "perimeter error: -1.8514\n",
      "mean area: -1.8667\n",
      "mean texture: -1.9910\n",
      "worst concavity: -2.0364\n",
      "radius error: -2.4517\n",
      "worst perimeter: -2.7465\n",
      "worst area: -2.7538\n",
      "mean concavity: -2.9788\n",
      "worst radius: -3.0011\n",
      "worst texture: -3.1849\n",
      "worst concave points: -3.9194\n",
      "mean concave points: -4.1637\n"
     ]
    }
   ],
   "source": [
    "#running logit regression \n",
    "\n",
    "data = load_breast_cancer()\n",
    "#print(data.data.shape), output was (569,30)\n",
    "\n",
    "y = data.target \n",
    "X = data.data #other numerical features \n",
    "\n",
    "#normalising\n",
    "y = (y - y.min()) / (y.max() - y.min())\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "#logit regression\n",
    "theta_opt, X_norm, y_norm = optimizer_f(X, y, alpha=0.1, iterations=10000) #using max of 10000 iterations\n",
    "\n",
    "#final equation\n",
    "feature_names = [\"Intercept\"] + list(data.feature_names)\n",
    "equation = \" + \".join(f\"{theta_opt[i]:.4f}*{feature_names[i]}\" for i in range(len(theta_opt)))\n",
    "\n",
    "#ranking coefficients from pos to neg\n",
    "coef_ranking = sorted(zip(feature_names, theta_opt), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "print(f\"\\nFinal Logistic Regression Equation:\\n{equation}\\n\")\n",
    "print(\"Feature Coefficients (Ranked from Positive to Negative):\")\n",
    "for feature, coef in coef_ranking:\n",
    "    print(f\"{feature}: {coef:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final Logistic Regression Equation: \n",
    "9.7400*(Intercept) + -1.2493*(mean radius) + -1.9910*(mean texture) + -1.3578*(mean perimeter) + -1.8667*(mean area) + 0.3557*(mean smoothness) + -0.4573*(mean compactness) + -2.9788*(mean concavity) + -4.1637*(mean concave points) + 0.3137*(mean symmetry) + 2.6421*(mean fractal dimension) + -2.4517*(radius error) + 0.3583*(texture error) + -1.8514*(perimeter) error + -1.7106*(area error) + 0.3479*(smoothness error) + 1.5651*(compactness error) + 0.7751*(concavity error) + 0.3495*(concave points error) + 0.8047*(symmetry error) + 1.3182*(fractal dimension error) + -3.0011*(worst radius) + -3.1849*(worst texture) + -2.7465*(worst perimeter) + -2.7538*(worst area) + -1.6942*(worst smoothness) + -1.0459*(worst compactness) + -2.0364*(worst concavity) + -3.9194*(worst concave points) + -1.6949*(worst symmetry) + -0.2300*(worst fractal dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Coefficients (Ranked from Positive to Negative):\n",
    "\n",
    "Intercept: 9.7400\n",
    "\n",
    "mean fractal dimension: 2.6421\n",
    "\n",
    "compactness error: 1.5651\n",
    "\n",
    "fractal dimension error: 1.3182\n",
    "\n",
    "symmetry error: 0.8047\n",
    "\n",
    "concavity error: 0.7751\n",
    "\n",
    "texture error: 0.3583\n",
    "\n",
    "mean smoothness: 0.3557\n",
    "\n",
    "concave points error: 0.3495\n",
    "\n",
    "smoothness error: 0.3479\n",
    "\n",
    "mean symmetry: 0.3137\n",
    "\n",
    "worst fractal dimension: -0.2300\n",
    "\n",
    "mean compactness: -0.4573\n",
    "\n",
    "worst compactness: -1.0459\n",
    "\n",
    "mean radius: -1.2493\n",
    "\n",
    "mean perimeter: -1.3578\n",
    "\n",
    "worst smoothness: -1.6942\n",
    "\n",
    "worst symmetry: -1.6949\n",
    "\n",
    "area error: -1.7106\n",
    "\n",
    "perimeter error: -1.8514\n",
    "\n",
    "mean area: -1.8667\n",
    "\n",
    "mean texture: -1.9910\n",
    "\n",
    "worst concavity: -2.0364\n",
    "\n",
    "radius error: -2.4517\n",
    "\n",
    "worst perimeter: -2.7465\n",
    "\n",
    "worst area: -2.7538\n",
    "\n",
    "mean concavity: -2.9788\n",
    "\n",
    "worst radius: -3.0011\n",
    "\n",
    "worst texture: -3.1849\n",
    "\n",
    "worst concave points: -3.9194\n",
    "\n",
    "mean concave points: -4.1637\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# q10 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Fold CV MSE: 0.022460523225369913\n",
      "Leave-One-Out CV MSE: 0.022456875235560322\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, LeaveOneOut\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "model = LinearRegression()\n",
    "random_seed = 265\n",
    "# K-Fold Cross Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=random_seed)\n",
    "kf_mse = cross_val_score(model, X, Y, cv=kf, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Leave-One-Out Cross Validation\n",
    "loo = LeaveOneOut()\n",
    "loo_mse = cross_val_score(model, X, Y, cv=loo, scoring='neg_mean_squared_error')\n",
    "\n",
    "\n",
    "# Print MSE for each strategy\n",
    "print(f'K-Fold CV MSE: {-kf_mse.mean()}')\n",
    "print(f'Leave-One-Out CV MSE: {-loo_mse.mean()}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Test Split (avg over 10 runs) MSE: 0.023056\n"
     ]
    }
   ],
   "source": [
    "# train-test split cross-validation (repeat 10 times for stability)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "num_splits = 10\n",
    "test_size = 0.2\n",
    "\n",
    "mse_scores = []\n",
    "\n",
    "for _ in range(num_splits):\n",
    "    # split data into training and test sets\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=random_seed)\n",
    "\n",
    "    # train the model\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    # predict on the test set\n",
    "    Y_pred = model.predict(X_test)\n",
    "\n",
    "    # calculate MSE\n",
    "    tt_mse = mean_squared_error(Y_test, Y_pred)\n",
    "    mse_scores.append(tt_mse)\n",
    "\n",
    "# average MSE across splits\n",
    "train_test_split_mse = np.mean(mse_scores)\n",
    "print(f'Train-Test Split (avg over {num_splits} runs) MSE: {train_test_split_mse:.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model trained on full data MSE: 0.022290\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, Y)\n",
    "y_pred = model.predict(X)\n",
    "final_mse = mean_squared_error(Y, y_pred)\n",
    "print(f'Final model trained on full data MSE: {final_mse:.6f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

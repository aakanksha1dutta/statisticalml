{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# q3\n",
    "From scratch (not using any pre-packaged tools for direct optimization), implement the\n",
    "stochastic gradient descent algorithm for linear regression and test your results on the\n",
    "California Housing Prices Dataset (you can implement simple matrix operations by using\n",
    "a package like numpy):\n",
    "- https://scikit-\n",
    "learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html#skl\n",
    "earn.datasets.fetch_california_housing\n",
    "Here is what you need to do step by step:\n",
    "* Implement the stochastic gradient descent algorithm from scratch\n",
    "\n",
    "* Choose the following features from the dataset as your X matrix: MedInc,\n",
    "HouseAge, AveRooms, AveBedrms, Population, AveOccup, Latitude, Longitude\n",
    "\n",
    "* Choose the following feature from the dataset as your Y matrix: MedHouseVal\n",
    "\n",
    "* Apply 0 – 1 normalization on X and Y.\n",
    "\n",
    "* Randomly split your data into training (70% of total) and test sets (30% of total)\n",
    "by using sklearn’s train_test_split function. Set random_state = 265:\n",
    "https://scikit-\n",
    "learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.ht\n",
    "ml.\n",
    "* Use the ‘ideal’ learning rate and number of steps values [‘ideal’ means that\n",
    "you can make the decision based on the computational power you have].\n",
    "* By running your code, determine the best set of parameters (=weights) for the\n",
    "constant and your features listed in b). Your cost function will be MSE (=you should\n",
    "pick the set of parameters that give you the lowest MSE).\n",
    "\n",
    "* Report and interpret the results. \n",
    "* What are the factors that explain the house\n",
    "prices the most?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_california_housing()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "Y = data.target\n",
    "\n",
    "\n",
    "features = [\"MedInc\", \"HouseAge\", \"AveRooms\", \"AveBedrms\", \"Population\", \"AveOccup\", \"Latitude\", \"Longitude\"]\n",
    "X = X[features]\n",
    "\n",
    "# 0-1 normalization\n",
    "X = (X - X.min()) / (X.max() - X.min())\n",
    "Y = (Y - Y.min()) / (Y.max() - Y.min())\n",
    "\n",
    "\n",
    "X = X.to_numpy()\n",
    "Y = Y.reshape(-1, 1)\n",
    "\n",
    "# bias terms\n",
    "X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=265)\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(X, Y, learning_rate=0.01, epochs=1000):\n",
    "    m, n = X.shape\n",
    "    weights = np.zeros((n, 1))  # Initialize weights\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(m):\n",
    "            idx = np.random.randint(0, m)  \n",
    "            x_i = X[idx].reshape(1, -1)\n",
    "            y_i = Y[idx].reshape(1, -1)\n",
    "            \n",
    "            prediction = np.dot(x_i, weights)\n",
    "            error = prediction - y_i\n",
    "            \n",
    "            # Update weights using SGD update rule\n",
    "            weights -= learning_rate * x_i.T @ error\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.02204339920941451\n",
      "Testing MSE: 0.023050721944464954\n",
      "Learned Weights: [ 0.73996337  1.30962198  0.10408374 -3.01055051  4.72020398 -0.06997644\n",
      " -0.82964972 -0.82005066 -0.91684462]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "weights = stochastic_gradient_descent(X_train, Y_train, learning_rate, epochs)\n",
    "\n",
    "# Predict function\n",
    "def predict(X, weights):\n",
    "    return np.dot(X, weights)\n",
    "\n",
    "# Compute Mean Squared Error (MSE)\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Evaluate the model\n",
    "y_train_pred = predict(X_train, weights)\n",
    "y_test_pred = predict(X_test, weights)\n",
    "\n",
    "mse_train = mse(Y_train, y_train_pred)\n",
    "mse_test = mse(Y_test, y_test_pred)\n",
    "\n",
    "print(f\"Training MSE: {mse_train}\")\n",
    "print(f\"Testing MSE: {mse_test}\")\n",
    "print(f\"Learned Weights: {weights.ravel()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important Factors:\n",
    "* population (+4.7874) → strong positive effect.\n",
    "* average bedrooms per dwelling (-2.9529) → more bedrooms seem to lower house prices, likely due to smaller rooms.\n",
    "* house age (+1.3161) → older houses have higher prices, possibly due to location.\n",
    "* median income (+0.7355) → wealthier areas have more expensive homes.\n",
    "* latitude & longitude (-0.83, -0.82) → location matters significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_training mse_ = 0.0220, _testing mse_ = 0.0231\n",
    "* since the training and testing mse values are very close, this means our model generalizes well.\n",
    "* the small mse values suggest that our model is making relatively accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# q4\n",
    "* Use SGDRegressor provided by scikit:\n",
    "https://scikit-\n",
    "learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html\n",
    "* Step b, c, d, and e are the same as in Question 3.\n",
    "* Set random_state = 265, and loss = ‘squared_error’. Use the ‘ideal’\n",
    "learning rate and number of steps values [‘ideal means that you can make\n",
    "the decision based on the computational power you have]. These two parameters\n",
    "should be the same as those you used in Question 3. Other parameters should be\n",
    "set to ‘default’.\n",
    "By running your code, determine the best set of parameters (=weights) for the\n",
    "constant and your features listed in b).\n",
    "* Report and interpret the results. What are the factors that explain the house\n",
    "prices the most? Are the results different from the previous question? If\n",
    "different, explain why the results might be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SGDRegressor Results:\n",
      "Training MSE: 0.022833223249611876\n",
      "Testing MSE: 0.023289385971676832\n",
      "Learned Weights: [ 0.25506976  0.23673588  0.23673588  1.13991477  0.11819594  0.07999486\n",
      "  0.15911785 -0.01362413 -0.07618674 -0.77718429 -0.85884423]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=265)\n",
    "X_train = np.c_[np.ones(X_train.shape[0]), X_train]\n",
    "X_test = np.c_[np.ones(X_test.shape[0]), X_test]\n",
    "\n",
    "# Hyperparameters \n",
    "learning_rate = 0.01\n",
    "num_epochs = 1000\n",
    "n_samples, n_features = X_train.shape \n",
    "weights = np.random.randn(n_features, 1)\n",
    "\n",
    "# Using Scikit-Learn's SGDRegressor\n",
    "sgd_reg = SGDRegressor(loss='squared_error', learning_rate='constant', eta0=learning_rate, max_iter=num_epochs, random_state=265)\n",
    "sgd_reg.fit(X_train, y_train.ravel())\n",
    "\n",
    "# Compute MSE\n",
    "train_mse_sklearn = np.mean((sgd_reg.predict(X_train) - y_train.ravel()) ** 2)\n",
    "test_mse_sklearn = np.mean((sgd_reg.predict(X_test) - y_test.ravel()) ** 2)\n",
    "\n",
    "print(\"\\nSGDRegressor Results:\")\n",
    "print(f\"Training MSE: {train_mse_sklearn}\")\n",
    "print(f\"Testing MSE: {test_mse_sklearn}\")\n",
    "print(f\"Learned Weights: {np.hstack((sgd_reg.intercept_.reshape(-1), sgd_reg.coef_))}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the Weights:\n",
    "AveBedrms (4.787) has the strongest positive impact on house prices, meaning more bedrooms per household correlate with higher prices.\n",
    "HouseAge (1.316) also has a strong positive effect, meaning older houses might be in established neighborhoods with higher demand.\n",
    "Population (-2.952) has the strongest negative impact, indicating that densely populated areas tend to have lower house prices.\n",
    "Longitude (-0.904) and Latitude (-0.820) have negative coefficients, suggesting a geographical pricing pattern (e.g., inland locations may be less expensive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very similar result as our custom Stochastic Gradient Descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# q10 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10) Using the pre-packaged cross-validation functions implemented in the scikit package,\n",
    "provide a classification by using the California Housing Prices dataset, and answer the\n",
    "following questions:\n",
    "Dataset: https://scikit-\n",
    "learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html#skl\n",
    "earn.datasets.fetch_california_housing\n",
    "* Choose the following features from the dataset as your X matrix: MedInc,\n",
    "HouseAge, AveRooms, AveBedrms, Population, AveOccup, Latitude, Longitude\n",
    "* Choose the following feature from the dataset as your Y matrix: MedHouseVal\n",
    "* Apply 0 – 1 normalization on X and Y.\n",
    "* Apply the three different cross-validation strategies to train your model. (For\n",
    "splitting your data always use 265 as your random number or seed value).\n",
    "* Now, using scikit’s sklearn.linear_model.LinearRegression, predict the\n",
    "house prices by using all of the data in your X matrix. Compare the performance\n",
    "obtained with different techniques of CV. Which CV strategy provides the lowest\n",
    "MSE? Why? Interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Fold CV MSE: 0.022460523225369913\n",
      "Leave-One-Out CV MSE: 0.022456875235560322\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, LeaveOneOut\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "model = LinearRegression()\n",
    "random_seed = 265\n",
    "# K-Fold Cross Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=random_seed)\n",
    "kf_mse = cross_val_score(model, X, Y, cv=kf, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Leave-One-Out Cross Validation\n",
    "loo = LeaveOneOut()\n",
    "loo_mse = cross_val_score(model, X, Y, cv=loo, scoring='neg_mean_squared_error')\n",
    "\n",
    "\n",
    "# Print MSE for each strategy\n",
    "print(f'K-Fold CV MSE: {-kf_mse.mean()}')\n",
    "print(f'Leave-One-Out CV MSE: {-loo_mse.mean()}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Test Split (avg over 10 runs) MSE: 0.023056\n"
     ]
    }
   ],
   "source": [
    "# train-test split cross-validation (repeat 10 times for stability)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "num_splits = 10\n",
    "test_size = 0.2\n",
    "\n",
    "mse_scores = []\n",
    "\n",
    "for _ in range(num_splits):\n",
    "    # split data into training and test sets\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=random_seed)\n",
    "\n",
    "    # train the model\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    # predict on the test set\n",
    "    Y_pred = model.predict(X_test)\n",
    "\n",
    "    # calculate MSE\n",
    "    tt_mse = mean_squared_error(Y_test, Y_pred)\n",
    "    mse_scores.append(tt_mse)\n",
    "\n",
    "# average MSE across splits\n",
    "train_test_split_mse = np.mean(mse_scores)\n",
    "print(f'Train-Test Split (avg over {num_splits} runs) MSE: {train_test_split_mse:.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model trained on full data MSE: 0.022290\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, Y)\n",
    "y_pred = model.predict(X)\n",
    "final_mse = mean_squared_error(Y, y_pred)\n",
    "print(f'Final model trained on full data MSE: {final_mse:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
